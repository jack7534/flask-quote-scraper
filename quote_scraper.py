import requests
from bs4 import BeautifulSoup
import json
import re
import math  # ğŸ”¹ å¼•å…¥ math æ¨¡çµ„ä¾†è™•ç†ç„¡æ¢ä»¶é€²ä½

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

def clean_price(price_text):
    """ æ¸…ç†åƒ¹æ ¼å­—ä¸²ï¼Œç¢ºä¿èƒ½è½‰æ›ç‚º int """
    price_text = price_text.replace("å††", "").replace(",", "").replace("(ç¨è¾¼)", "").replace("(ç¨ 0)", "").strip()
    return int(re.sub(r"[^\d]", "", price_text))  # åªä¿ç•™æ•¸å­—éƒ¨åˆ†ï¼Œç§»é™¤æ—¥æ–‡å­—

def scrape_amazon_japan(url):
    """ çˆ¬å– Amazon Japan å•†å“è³‡è¨Š """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, "lxml")

        title = soup.select_one("#productTitle")
        price = soup.select_one(".a-price .a-offscreen")

        if title and price:
            price_jpy = clean_price(price.text)
            return {
                "ç¶²ç«™": "Amazon Japan",
                "åç¨±": title.text.strip(),
                "æ—¥å¹£åƒ¹æ ¼": price_jpy,
                "å°å¹£å ±åƒ¹": math.ceil(price_jpy * 0.35),  # ğŸ”¹ ç„¡æ¢ä»¶é€²ä½
                "é€£çµ": url
            }
        return {"éŒ¯èª¤": "ç„¡æ³•ç²å– Amazon å•†å“åƒ¹æ ¼"}
    except Exception as e:
        return {"éŒ¯èª¤": f"Amazon çˆ¬å–å¤±æ•—: {str(e)}"}

def get_quotation(url):
    """ æ ¹æ“šæä¾›çš„ç¶²å€ï¼Œé¸æ“‡å°æ‡‰çš„çˆ¬èŸ² """
    if "amazon.co.jp" in url:
        return scrape_amazon_japan(url)
    else:
        return {"éŒ¯èª¤": "ç›®å‰ä¸æ”¯æ´æ­¤ç¶²ç«™"}

if __name__ == "__main__":
    # ğŸš¨ âŒ é€™ä¸€æ®µåˆªé™¤ï¼Œå› ç‚º API æœƒèª¿ç”¨é€™å€‹å‡½æ•¸ï¼Œè€Œä¸æ˜¯é€é input()
    # url = input("ğŸ” è«‹è¼¸å…¥å•†å“ç¶²å€ï¼š")
    # result = get_quotation(url)
    # print(json.dumps(result, ensure_ascii=False, indent=2))
    pass  # ä¿ç•™é€™å€‹ä½œç‚ºç©ºç™½åŸ·è¡Œ
