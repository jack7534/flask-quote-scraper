import requests
from bs4 import BeautifulSoup
import json
import re
import math
import time
import random
import cloudscraper  # éœ€è¦å®‰è£ `pip install cloudscraper`

# è¨­å®š User-Agent é¿å…è¢«æ“‹
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# **å…¨å±€å®šç¾© session**
session = requests.Session()

def clean_price(price_text):
    """ æ¸…ç†åƒ¹æ ¼å­—ä¸²ï¼Œç¢ºä¿èƒ½è½‰æ›ç‚º int """
    price_text = re.sub(r"[^\d]", "", price_text.replace("å††", "").replace(",", "").strip())
    return int(price_text) if price_text else None

### **é€™è£¡æ”¹å› 01 ç‰ˆæœ¬çš„ Amazon Japan**
def scrape_amazon_japan(url):
    """ çˆ¬å– Amazon Japan å•†å“è³‡è¨Š """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, "lxml")

        title = soup.select_one("#productTitle")
        price = soup.select_one(".a-price .a-offscreen")
        image = soup.select_one("#landingImage")

        price_jpy = clean_price(price.text) if price else None
        image_url = image["src"] if image else ""

        if title and price_jpy:
            return {
                "ç¶²ç«™": "Amazon Japan",
                "åç¨±": title.text.strip(),
                "æ—¥å¹£åƒ¹æ ¼": price_jpy,
                "å°å¹£å ±åƒ¹": math.ceil(price_jpy * 0.35),
                "åœ–ç‰‡": image_url,
                "é€£çµ": url
            }
        return {"éŒ¯èª¤": "ç„¡æ³•ç²å– Amazon å•†å“åƒ¹æ ¼"}
    except Exception as e:
        return {"éŒ¯èª¤": f"Amazon çˆ¬å–å¤±æ•—: {str(e)}"}

### **ä»¥ä¸‹çš„ Rakutenã€Yahooã€Bic Camera éƒ½å®Œå…¨ä¸å‹•**
def scrape_rakuten(url):
    """ çˆ¬å– Rakuten æ¨‚å¤©å¸‚å ´ å•†å“è³‡è¨Š """
    try:
        response = session.get(url, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.text, "lxml")

        title = soup.select_one(".item-name") or soup.select_one("h1") or soup.select_one("meta[property='og:title']")
        title_text = title["content"].strip() if title and title.name == "meta" else title.text.strip() if title else "ç„¡æ³•ç²å–å•†å“åç¨±"

        price_meta = soup.select_one("meta[itemprop='price']")
        price_jpy = clean_price(price_meta["content"]) if price_meta else None

        image = soup.select_one("meta[property='og:image']")
        image_url = image["content"] if image else ""

        if price_jpy:
            return {
                "ç¶²ç«™": "Rakuten",
                "åç¨±": title_text,
                "æ—¥å¹£åƒ¹æ ¼": price_jpy,
                "å°å¹£å ±åƒ¹": math.ceil(price_jpy * 0.35),
                "åœ–ç‰‡": image_url,
                "é€£çµ": url
            }
        return {"éŒ¯èª¤": "ç„¡æ³•ç²å– Rakuten å•†å“åƒ¹æ ¼"}
    except Exception as e:
        return {"éŒ¯èª¤": f"Rakuten çˆ¬å–å¤±æ•—: {str(e)}"}

def scrape_yahoo_auction(url):
    """ çˆ¬å– Yahoo Auctions å•†å“è³‡è¨Š """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, "lxml")

        title = soup.select_one(".Product__title") or soup.select_one(".ProductTitle__text")
        bid_price = soup.select_one(".Price__value")
        buy_price = soup.select_one(".Price__now") or soup.select_one(".ProductPrice__value")
        auction_time = soup.select_one(".Auction__endTime")

        price_jpy = None
        price_text = None

        if buy_price:
            price_text = buy_price.text.strip()
        elif bid_price:
            price_text = bid_price.text.strip()

        if price_text:
            price_text = re.sub(r"ç¨\s*\d*\s*å††", "", price_text)
            price_jpy = clean_price(price_text)

        image = soup.select_one("meta[property='og:image']")
        image_url = image["content"] if image else ""

        if title and price_jpy:
            return {
                "ç¶²ç«™": "Yahoo Auctions",
                "åç¨±": title.text.strip(),
                "æ—¥å¹£åƒ¹æ ¼": price_jpy,
                "å°å¹£å ±åƒ¹": math.ceil(price_jpy * 0.35),
                "ç«¶æ¨™çµæŸæ™‚é–“": auction_time.text.strip() if auction_time else "ç„¡æ³•å–å¾—",
                "åœ–ç‰‡": image_url,
                "é€£çµ": url
            }
        return {"éŒ¯èª¤": "ç„¡æ³•ç²å– Yahoo Auctions åƒ¹æ ¼"}
    except Exception as e:
        return {"éŒ¯èª¤": f"Yahoo Auctions çˆ¬å–å¤±æ•—: {str(e)}"}


def scrape_bic_camera(url):
    """ çˆ¬å– Bic Camera å•†å“è³‡è¨Š """
    try:
        scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False})
        session = requests.Session()

        # å…ˆè¨ªå•é¦–é ï¼Œå–å¾— Cookies
        session.get("https://www.biccamera.com/", headers=HEADERS, timeout=10)

        # ç™¼é€è«‹æ±‚
        response = scraper.get(url, headers=HEADERS, timeout=30)

        if response.status_code != 200:
            return {"éŒ¯èª¤": f"Bic Camera è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"}

        soup = BeautifulSoup(response.text, "lxml")

        # å•†å“åç¨±
        title = soup.select_one("h1")
        title_text = title.text.strip() if title else "ç„¡æ³•ç²å–å•†å“åç¨±"

        # åƒ¹æ ¼
        price_meta = soup.select_one('meta[itemprop="price"]')
        price_jpy = clean_price(price_meta["content"]) if price_meta else None

        # åœ–ç‰‡
        image = soup.select_one("meta[property='og:image']")
        image_url = image["content"] if image else ""

        if title_text and price_jpy:
            return {
                "ç¶²ç«™": "Bic Camera",
                "åç¨±": title_text,
                "æ—¥å¹£åƒ¹æ ¼": price_jpy,
                "å°å¹£å ±åƒ¹": math.ceil(price_jpy * 0.35),
                "åœ–ç‰‡": image_url,
                "é€£çµ": url
            }
        return {"éŒ¯èª¤": "ç„¡æ³•ç²å– Bic Camera å•†å“åƒ¹æ ¼"}
    except Exception as e:
        return {"éŒ¯èª¤": f"Bic Camera çˆ¬å–å¤±æ•—: {str(e)}"}


### **æ–°å¢ Matsukiyo Cocokara**
def scrape_matsukiyo(url):
    """ çˆ¬å– Matsukiyo Cocokaraï¼ˆæ¾æœ¬æ¸…ï¼‰å•†å“è³‡è¨Š """
    try:
        # ä½¿ç”¨ cloudscraper ç¹éé˜²çˆ¬
        scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False})
        scraper.get("https://www.matsukiyococokara-online.com/", headers=HEADERS)  # å…ˆè¨ªå•é¦–é å–å¾— cookies
        response = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)

        # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 200ï¼Œå‰‡è¿”å›éŒ¯èª¤
        if response.status_code != 200:
            return {"éŒ¯èª¤": f"Matsukiyo è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"}

        soup = BeautifulSoup(response.text, "lxml")

        # å•†å“åç¨±
        title = soup.select_one("h1")
        title_text = title.text.strip() if title else "ç„¡æ³•ç²å–å•†å“åç¨±"

        # å˜—è©¦å¾ <div class='p-productdetail__price'> å–å¾—åƒ¹æ ¼
        price_meta = soup.select_one("div.p-productdetail__price big")
        price_jpy = clean_price(price_meta.text) if price_meta else None

        # åœ–ç‰‡
        image = soup.select_one("meta[property='og:image']")
        image_url = image["content"] if image else ""

        if title_text and price_jpy:
            return {
                "ç¶²ç«™": "Matsukiyo Cocokara",
                "åç¨±": title_text,
                "æ—¥å¹£åƒ¹æ ¼": price_jpy,
                "å°å¹£å ±åƒ¹": math.ceil(price_jpy * 0.35),
                "åœ–ç‰‡": image_url,
                "é€£çµ": url
            }
        return {"éŒ¯èª¤": "ç„¡æ³•ç²å– Matsukiyo å•†å“åƒ¹æ ¼"}
    except Exception as e:
        return {"éŒ¯èª¤": f"Matsukiyo çˆ¬å–å¤±æ•—: {str(e)}"}

def get_quotation(url):
    """ æ ¹æ“šæä¾›çš„ç¶²å€ï¼Œé¸æ“‡å°æ‡‰çš„çˆ¬èŸ² """
    if "amazon.co.jp" in url:
        return scrape_amazon_japan(url)
    elif "rakuten.co.jp" in url:
        return scrape_rakuten(url)
    elif "auctions.yahoo.co.jp" in url:
        return scrape_yahoo_auction(url)
    elif "biccamera.com" in url:
        return scrape_bic_camera(url)
    elif "matsukiyococokara-online.com" in url:
        return scrape_matsukiyo(url)
    else:
        return {"éŒ¯èª¤": "ç›®å‰ä¸æ”¯æ´æ­¤ç¶²ç«™"}

if __name__ == "__main__":
    url = input("ğŸ” è«‹è¼¸å…¥å•†å“ç¶²å€ï¼š")
    result = get_quotation(url)
    print("\nğŸ“Œ å ±åƒ¹çµæœï¼š", json.dumps(result, indent=4, ensure_ascii=False))
